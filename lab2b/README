NAME: Naim Ayat
EMAIL: naimayat@ucla.edu
ID: 000000000
This is my submission for Lab 2B (Lock Granularity and Performance).
It is a tarball containing the following files:

	lab2_list.c: implements the specified command line options (--threads,
	--iterations, --yield, --sync, --lists), drives one or more parallel threads
	that do operations on a shared linked list, and reports on the final list and
	performance.

	SortedList.h: describes the interfaces for linked list operations.

	SortedList.c: implements insert, delete, lookup, and length methods for a
	sorted doubly linked list

	lab2b_list.csv: contains results for test runs.

	profile.out: report showing where time was spent in the un-partitioned
	spin-lock implementation.

	tests.sh: runs tests lab2_add and lab2_list, then records results in .csv.

	lab2b_1.png: graph of throughput vs. number of threads for mutex and spin-lock
	synchronized list operations.
	lab2b_2.png: graph of mean time per mutex wait and mean time per operation for
	mutex-synchronized list operations.
	lab2b_3.png: graph of successful iterations vs. threads for each
	synchronization method.
	lab2b_4.png: graph of throughput vs. number of threads for mutex synchronized
	partitioned lists.
	lab2b_5.png: graph of throughput vs. number of threads for
	spin-lock-synchronized partitioned lists.

	Makefile: builds the deliverable with the following targets:
		default: the lab2_list executable.
		tests: runs all specified test cases to generate CSV results.
		profile: runs tests with profiling tools to yield an execution profiling
		report.
		graphs: uses gnuplot to generate the required graphs.
		dist: creates the deliverable tarball.
		clean: deletes all programs and output generated by the Makefile.

	README: describes deliverable contents and answers spec questions.
________________________________________________________________________________

QUESTION 2.3.1 - Cycles in the basic list implementation:

	Where do you believe most of the cycles are spent in the 1 and 2-thread list
	tests?
		Most of the cycles are spent on linked list operations, like insert, delete,
		and lookup.
	Why do you believe these to be the most expensive parts of the code?
		They require a high number of iterations.
	Where do you believe most of the time/cycles are being spent in the
	high-thread spin-lock tests?
		Most of the time/cycles are spent spinning; since there is a high number of
		threads, it becomes increasingly probable that a given list will be locked
		by another thread.
	Where do you believe most of the time/cycles are being spent in the
	high-thread mutex tests?
			Most of the time/cycles will be spent on list operations; mutexes ensure
			that threads do not consume CPU cycles if they cannot access critical
			sections of the code.

QUESTION 2.3.2 - Execution Profiling:

	Where (what lines of code) are consuming most of the cycles when the spin-lock
	version of the list exerciser is run with a large number of threads?
		while (__sync_lock_test_and_set(&mlock[hashcont[k]], 1));
	Why does this operation become so expensive with large numbers of threads?
		Every thread uses CPU cycles on this operation except for the one actually
		holding the lock.

QUESTION 2.3.3 - Mutex Wait Time:

	Why does the average lock-wait time rise so dramatically with the number of
	contending threads?
		As the number of threads increases (and thus the number of contending
		threads), each thread must wait longer per lock on average. This is simply
		because there is a larger number of threads waiting on the same lock.
		Moreover, as the number of threads increases, the costliness of overhead
		switching increases as well. This also adds to lock-wait time.
	Why does the completion time per operation rise (less dramatically) with the
	number of contending threads?
		As the number of contenting threads increases, context switching overhead
		increases as threads have to check for open locks more frequently. It is
		less dramatic in this case since threads yield when they do not encounter
		an open lock, allowing another thread to check or execute.
	How is it possible for the wait time per operation to go up faster (or higher)
	than the completion time per operation?
		Again, threads yield when they do not encounter an open lock. Although the
		number of contending threads increases wait time, threads not waiting for
		the lock are still able to run. Therefore, wait time can go up faster than
		completion time.

QUESTION 2.3.4 - Performance of Partitioned Lists:

	Explain the change in performance of the synchronized methods as a function of
	the number of lists.
		As the number of lists increase, the lower the probability of thread
		contention. Thus, the performance of the synchronized methods generally
		improves as we add more lists.
	Should the throughput continue increasing as the number of lists is further
	increased? If not, explain why not.
		No. As the number of lists increases, the overhead from context switches
		increases as well. This is because there will be more subsections of lists
		in contention; in turn, more threads will spend time waiting for subsections
		to be released. Therefore, the wait time for a given subsection to free up
		will be longer than the time it takes to actually perform operations within
		the lists.
	It seems reasonable to suggest the throughput of an N-way partitioned list
	should be equivalent to the throughput of a single list with fewer (1/N)
	threads. Does this appear to be true in the above curves? If not, explain why
	not.
		No. As the number of threads waiting on locks for critical sections and
		increases, the amount of context switching between contending threads does
		as well. Both of these generate significant overhead to the extent that the
		potential performance improvement from parallelization cancels out.
		Therefore, we conclude that single lists with fewer threads produce better
		throughput than an N-way partitioned list.
